{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CAP 6610 Machine Learning\n",
    "\n",
    "#### Homework 1\n",
    "\n",
    "1.$\\space$a)$\\space$ The distribution is normalized: $\\space$ $\\int_{a}^{b} p(x;a,b) dx = \\int_{a}^{b} \\frac{1}{b-a} dx = \\frac{b-a}{b-a} = 1$\n",
    "\n",
    "$\\space$b) Mean: $\\mathrm{E}[X] =\\int_{-\\infty}^{\\infty} x p(x;a,b) d x =\\int_{a}^{b} x \\frac{1}{b-a} d x =\\frac{1}{b-a} \\int_{a}^{b} x d x =\\frac{1}{b-a}\\left[\\frac{1}{2} x^{2}\\right]_{a}^{b} =\\frac{1}{b-a} \\frac{1}{2}\\left[b^{2}-a^{2}\\right] =\\frac{(b-a)(b+a)}{2(b-a)} =\\frac{b+a}{2}$\n",
    "\n",
    "$\\space$c) Variance: $\\mathrm{E}\\left[x^{2}\\right] =\\int_{-\\infty}^{\\infty} x^{2} p(x;a,b) d x =\\int_{a}^{b} x^{2} \\frac{1}{b-a} d x =\\frac{1}{b-a} \\int_{a}^{b} x^{2} d x =\\frac{1}{b-a}\\left[\\frac{1}{3} x^{3}\\right]_{a}^{b} =\\frac{1}{b-a} \\frac{1}{3}\\left[b^{3}-a^{3}\\right] =\\frac{(b-a)\\left(b^{2}+b a+a^{2}\\right)}{3(b-a)} =\\frac{b^{2}+b a+a^{2}}{3} =\\frac{(4-3) b^{2}+(4-6) b a+(4-3) a^{2}}{12} =\\frac{b^{2}-2 b a+b^{2}}{12} =\\frac{(b-a)^{2}}{12} $\n",
    "\n",
    "2.$\\space$ a) $\\space$ By knowing that there are at least one boy, it reduces the sample space from $\\{BG,GB,BB,GG\\}$ to $\\{BG,GB,BB\\}$,so $P(one\\space child \\space is \\space girl)=\\frac{2}{3}$ \n",
    "\n",
    "$\\space$ b) $\\space$ In $\\{BG,GB,BB,GG\\}$, there is $\\frac{1}{8}$ probability that the boy who ran past was the first B of BB, and $\\frac{1}{8}$ probability he was the second boy of BB, and $\\frac{1}{8}$ probability he was the boy of BG and $\\frac{1}{8}$ probability of being the boy of GB. 4 events with equal probability, so  $P(the\\space other \\space child \\space is \\space girl)=\\frac{1}{2}$ \n",
    "\n",
    "3.$\\space$\n",
    "\n",
    "$\\begin{aligned} \\operatorname{var}(X+Y) &=E[(X+Y)(X+Y)]-E[X+Y]^{2} \\\\ &=E\\left[X^{2}+2 X Y+Y^{2}\\right]-\\left(\\mu_{x}+\\mu_{y}\\right)^{2} \\\\ &=E\\left[X^{2}+2 X Y+Y^{2}\\right]-\\mu_{x}^{2}-2 \\mu_{x} \\mu_{y}-\\mu_{y}^{2} \\\\ &=\\left(E\\left[X^{2}\\right]-\\mu_{x}^{2}\\right)+\\left(E\\left[Y^{2}\\right]-\\mu_{y}^{2}\\right)+2\\left(E[X Y]-\\mu_{x} \\mu_{y}\\right) \\\\ &=\\operatorname{var}(X)+\\operatorname{var}(Y)+2 \\operatorname{cov}(X, Y) \\end{aligned}$\n",
    "\n",
    "4.$\\space$\n",
    "\n",
    "$\\begin{aligned} & \\operatorname{cov}(X, Y) \\\\=& E\\left(\\left(X-\\mu_{X}\\right)\\left(Y-\\mu_{Y}\\right)\\right) \\\\=& E\\left(X Y-\\mu_{X} Y-X \\mu_{Y}+\\mu_{X} \\mu_{Y}\\right) \\\\=& E(X Y)-\\mu_{X} E(Y)-E(X) \\mu_{Y}+\\mu_{X} \\mu_{Y} \\\\=& E(X Y)-\\mu_{X} \\mu_{Y} \\end{aligned}$\n",
    "\n",
    "If $X$ and $Y$ are independent variables, \n",
    "\n",
    "$\\operatorname{cov}(X,Y) = E(XY) - \\mu_{X} \\mu_{Y} = E(X)E(Y) - \\mu_{X} \\mu_{Y} = 0$\n",
    "\n",
    "5.$\\space$\n",
    "\n",
    "$\\space$a) \n",
    "\n",
    "$\\begin{aligned} E(x+z) &=\\sum_{x} \\sum_{z}(x+z) P_{x z}(x, z) \\\\ &=\\sum_{x} \\sum_{z} x P_{x z}(x, z)+\\sum_{z} \\sum_{x} z P_{x z}(x, z) \\\\ &=\\sum_{x} x P_{x}(x)+z P_{z}(z) \\\\ &=E(x)+E(z) \\end{aligned}$\n",
    "\n",
    "$\\space$b) Following derivation of question 3 and 4:\n",
    "\n",
    "$\\operatorname{var}(x+z) = \\operatorname{var}(x)+\\operatorname{var}(z)+2 \\operatorname{cov}(x,z) = \\operatorname{var}(x)+\\operatorname{var}(z) $ because $x$ and $z$ are statistically independent variables.\n",
    "\n",
    "6.$\\space$for $x_{1}=0, x_{2}=n$ and $x_{1}=1, x_{2}=n-1$\n",
    "\n",
    "$\\mathcal{F}\\left(0\\right)+\\alpha \\mathcal{F}\\left(n\\right)=\\mathcal{F}\\left(\\mathcal{F}\\left(n\\right)\\right)$ $\\space$ 1)\n",
    "\n",
    "$\\mathcal{F}\\left(\\alpha\\right)+\\alpha \\mathcal{F}\\left(n-1\\right)=\\mathcal{F}\\left(\\mathcal{F}\\left(n\\right)\\right)$ 2)\n",
    "\n",
    "from 1) and 2), $\\mathcal{F}\\left(\\alpha\\right)+\\alpha \\mathcal{F}\\left(n-1\\right) = \\mathcal{F}\\left(0\\right)+\\alpha \\mathcal{F}\\left(n\\right)$\n",
    "\n",
    "so, $\\sum_{n=1}^{n}\\mathcal{F}\\left(n\\right)-\\mathcal{F}\\left(n-1\\right) = \\mathcal{F}\\left(n\\right)-\\mathcal{F}\\left(0\\right) = n\\frac{\\mathcal{F}(\\alpha) - \\mathcal{F}(0)}{\\alpha}$\n",
    "\n",
    "so, $\\mathcal{F}(n)=kn+b, $ where $ k=\\frac{\\mathcal{F}(\\alpha) - \\mathcal{F}(0)}{\\alpha}; b=\\mathcal{F}\\left(0\\right)$\n",
    "\n",
    "$\\mathcal{G}=\\underset{f \\in \\mathcal{C}}{\\operatorname{argmin}} \\sum_{i=1}^{N}\\left(y_{i}-f\\left(x_{i}\\right)\\right)^{2}$ $=$$\\mathcal{G}=\\underset{f \\in \\mathcal{C}}{\\operatorname{argmin}} \\sum_{i=1}^{N}\\left(y_{i}-kx_i -b-\\epsilon\\right)^{2}$\n",
    "\n",
    "By satisfying $\\frac{dG}{dk} = 0;$ $\\frac{dG}{db} = 0$ and $\\frac{dG}{d\\epsilon} = 0$, we get $k=\\frac{\\sum_{i=1}^{N} y_{i}\\left(x_{i}-\\bar{x}\\right)}{\\sum_{i=1}^{k} x_{i}^{2}-\\frac{1}{n}\\left(\\sum_{i=1}^{N} x_{i}\\right)^{2}}$ and $b=\\frac{\\sum_{i=1}^{N}\\left(y_{i}-\\omega x_{i}\\right)}{N}-\\varepsilon$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "7.$\\space$ Because there exists an uncountable family of mutually disjoint Borel subsets of  $R$. If we consider subsets $F$ of $2^R$, branches of the tree $2^R$ are disjoint and Borel.\n",
    "\n",
    "8.$\\space$  a) $\\Omega=\\left\\{\\left(x_{1}, \\cdots, x_{n}\\right): x_{i} \\in\\{H, T\\}\\right\\}$\n",
    "$F = \\{\\emptyset, \\Omega\\}$ $F $ is $\\sigma$-algebra because:\n",
    "\n",
    "1) $\\Omega \\in \\mathcal{F}$ and $\\emptyset \\in \\mathcal{F}(\\emptyset$ denotes the empty set $)$\n",
    "\n",
    "2) If $A \\in \\mathcal{F}$ then $\\Omega \\backslash A=A^{c} \\in \\mathcal{F}:$ The complementary subset of $A$ is also in $\\Omega$\n",
    "\n",
    "3) For all $A_{i} \\in \\mathcal{F}: \\bigcup_{i=1}^{\\infty} A_{i} \\in \\mathcal{F}$\n",
    "\n",
    "b) $\\space$ $F$ is countable, because $\\Omega$ is countable, which means Head can be got with finite trials.\n",
    "\n",
    "c) $P(w_i) = p_i,$ where $w_i$ is $ (x_1, x_2,...,x_i) $. \n",
    "\n",
    "\n",
    "Because:\n",
    "\n",
    "$\\mathrm{P}(\\Omega)=1$\n",
    "\n",
    "If $A \\in \\Omega$, then $P(A) \\geq 0$\n",
    "\n",
    "If $A_{1}, A_{2}, A_{3}, \\ldots \\in \\Omega$ are mutually disjoint, then $\n",
    "P\\left(\\bigcup_{i=1}^{\\infty} A_{i}\\right)=\\sum_{i=1}^{\\infty} P\\left(A_{i}\\right)$\n",
    "\n",
    "d) Because the set $F$ is countable, so $\\lim _{x \\rightarrow+\\infty} P(X \\leq x) = P(\\Omega) = 1$\n",
    "\n",
    "Probability funtion: $p_{X}(x)=(1-p)^{x} p $, where $p \\in (0,1)$ means the probability of getting head using this biased coin.\n",
    "\n",
    "$\\mathrm{E}[X]$\n",
    "$=\\sum_{x=0}^{\\infty}(1-p)^{x} p x$\n",
    "$=(1-p) p \\sum_{x=0}^{\\infty}(1-p)^{x-1} x$\n",
    "$=-(1-p) p \\sum_{x=0}^{\\infty} \\frac{d}{d p}(1-p)^{x}$\n",
    "$=-(1-p) p \\frac{d}{d p} \\sum_{x=0}^{\\infty}(1-p)^{x}$\n",
    "$=-(1-p) p \\frac{d}{d p} \\frac{1}{1-(1-p)}$\n",
    "$=-(1-p) p \\frac{d}{d p} \\frac{1}{p}$\n",
    "$=-(1-p) p\\left(-p^{-2}\\right)$\n",
    "$=\\left(p-p^{2}\\right) p^{-2}$\n",
    "$=\\frac{1}{p}-1$\n",
    "$=\\frac{1-p}{p}$\n",
    "\n",
    "\n",
    "9.$\\space$ Let $A_{n}=\\left\\{X_{n} \\leq x\\right\\}$\n",
    "\n",
    "We get \n",
    "\n",
    "$\\liminf A_{n}=\\bigcup_{n \\geq 1} \\bigcap_{k \\geq n} A_{k}$, where $\\left\\{\\cap_{k \\geq n} A_{k}\\right\\}_{n=1}^{\\infty}$ is a sequence which increase to $lim \\space inf A_n$, \n",
    "\n",
    "then $P\\left(\\liminf A_{n}\\right)=\\lim _{n \\rightarrow \\infty} P\\left(\\bigcap_{k \\geq n} A_{k}\\right)\\space$ (1) following the continuity of $P$. \n",
    "\n",
    "Since $\\forall n, \\cap_{k \\geq n} A_{k} \\subseteq A_{n}$$\\rightarrow \\forall n, P\\left(\\cap_{k \\geq n} A_{k}\\right) \\leq P\\left(A_{n}\\right)$, so $\\liminf _{n \\rightarrow \\infty} P\\left(\\bigcap_{k \\geq n} A_{k}\\right) \\leq \\liminf _{n \\rightarrow \\infty} P\\left(A_{n}\\right)\\space$ (2)\n",
    "\n",
    "Combining  above (1) and (2), we get $P\\left(\\liminf _{n \\rightarrow \\infty} A_{n}\\right) \\leq \\liminf _{n \\rightarrow \\infty} P\\left(A_{n}\\right)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import initializers as init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "\n",
    "def my_init(shape, dtype=None):\n",
    "    return K.random_normal(shape, dtype=dtype)\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "from keras.optimizers import SGD\n",
    "import numpy as np\n",
    "\n",
    "model=Sequential()\n",
    "model.add(Dense(2, input_shape=(2,1),activation='sigmoid'))\n",
    "model.add(Dense(2, input_shape=(2,1),activation='sigmoid'))\n",
    "\n",
    "sgd = SGD(lr=0.1)\n",
    "model.compile(loss='mean_squared_error', optimizer=sgd)\n",
    "model.fit(np.array([1,1]).T, np.array([0,1]).T,  nb_epoch=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.291027774\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import math\n",
    "\n",
    "#\n",
    "# Shorthand:\n",
    "#   \"pd_\" as a variable prefix means \"partial derivative\"\n",
    "#   \"d_\" as a variable prefix means \"derivative\"\n",
    "#   \"_wrt_\" is shorthand for \"with respect to\"\n",
    "#   \"w_ho\" and \"w_ih\" are the index of weights from hidden to output layer neurons and input to hidden layer neurons respectively\n",
    "#\n",
    "# Comment references:\n",
    "#\n",
    "# [1] Wikipedia article on Backpropagation\n",
    "#   http://en.wikipedia.org/wiki/Backpropagation#Finding_the_derivative_of_the_error\n",
    "# [2] Neural Networks for Machine Learning course on Coursera by Geoffrey Hinton\n",
    "#   https://class.coursera.org/neuralnets-2012-001/lecture/39\n",
    "# [3] The Back Propagation Algorithm\n",
    "#   https://www4.rgu.ac.uk/files/chapter3%20-%20bp.pdf\n",
    "\n",
    "class NeuralNetwork:\n",
    "    LEARNING_RATE = 0.5\n",
    "\n",
    "    def __init__(self, num_inputs, num_hidden, num_outputs, hidden_layer_weights = None, hidden_layer_bias = None, output_layer_weights = None, output_layer_bias = None):\n",
    "        self.num_inputs = num_inputs\n",
    "\n",
    "        self.hidden_layer = NeuronLayer(num_hidden, hidden_layer_bias)\n",
    "        self.output_layer = NeuronLayer(num_outputs, output_layer_bias)\n",
    "\n",
    "        self.init_weights_from_inputs_to_hidden_layer_neurons(hidden_layer_weights)\n",
    "        self.init_weights_from_hidden_layer_neurons_to_output_layer_neurons(output_layer_weights)\n",
    "\n",
    "    def init_weights_from_inputs_to_hidden_layer_neurons(self, hidden_layer_weights):\n",
    "        weight_num = 0\n",
    "        for h in range(len(self.hidden_layer.neurons)):\n",
    "            for i in range(self.num_inputs):\n",
    "                if not hidden_layer_weights:\n",
    "                    self.hidden_layer.neurons[h].weights.append(random.random())\n",
    "                else:\n",
    "                    self.hidden_layer.neurons[h].weights.append(hidden_layer_weights[weight_num])\n",
    "                weight_num += 1\n",
    "\n",
    "    def init_weights_from_hidden_layer_neurons_to_output_layer_neurons(self, output_layer_weights):\n",
    "        weight_num = 0\n",
    "        for o in range(len(self.output_layer.neurons)):\n",
    "            for h in range(len(self.hidden_layer.neurons)):\n",
    "                if not output_layer_weights:\n",
    "                    self.output_layer.neurons[o].weights.append(random.random())\n",
    "                else:\n",
    "                    self.output_layer.neurons[o].weights.append(output_layer_weights[weight_num])\n",
    "                weight_num += 1\n",
    "\n",
    "    def inspect(self):\n",
    "        print('------')\n",
    "        print('* Inputs: {}'.format(self.num_inputs))\n",
    "        print('------')\n",
    "        print('Hidden Layer')\n",
    "        self.hidden_layer.inspect()\n",
    "        print('------')\n",
    "        print('* Output Layer')\n",
    "        self.output_layer.inspect()\n",
    "        print('------')\n",
    "\n",
    "    def feed_forward(self, inputs):\n",
    "        hidden_layer_outputs = self.hidden_layer.feed_forward(inputs)\n",
    "        return self.output_layer.feed_forward(hidden_layer_outputs)\n",
    "\n",
    "    # Uses online learning, ie updating the weights after each training case\n",
    "    def train(self, training_inputs, training_outputs):\n",
    "        self.feed_forward(training_inputs)\n",
    "\n",
    "        # 1. Output neuron deltas\n",
    "        pd_errors_wrt_output_neuron_total_net_input = [0] * len(self.output_layer.neurons)\n",
    "        for o in range(len(self.output_layer.neurons)):\n",
    "\n",
    "            # ∂E/∂zⱼ\n",
    "            pd_errors_wrt_output_neuron_total_net_input[o] = self.output_layer.neurons[o].calculate_pd_error_wrt_total_net_input(training_outputs[o])\n",
    "\n",
    "        # 2. Hidden neuron deltas\n",
    "        pd_errors_wrt_hidden_neuron_total_net_input = [0] * len(self.hidden_layer.neurons)\n",
    "        for h in range(len(self.hidden_layer.neurons)):\n",
    "\n",
    "            # We need to calculate the derivative of the error with respect to the output of each hidden layer neuron\n",
    "            # dE/dyⱼ = Σ ∂E/∂zⱼ * ∂z/∂yⱼ = Σ ∂E/∂zⱼ * wᵢⱼ\n",
    "            d_error_wrt_hidden_neuron_output = 0\n",
    "            for o in range(len(self.output_layer.neurons)):\n",
    "                d_error_wrt_hidden_neuron_output += pd_errors_wrt_output_neuron_total_net_input[o] * self.output_layer.neurons[o].weights[h]\n",
    "\n",
    "            # ∂E/∂zⱼ = dE/dyⱼ * ∂zⱼ/∂\n",
    "            pd_errors_wrt_hidden_neuron_total_net_input[h] = d_error_wrt_hidden_neuron_output * self.hidden_layer.neurons[h].calculate_pd_total_net_input_wrt_input()\n",
    "\n",
    "        # 3. Update output neuron weights\n",
    "        for o in range(len(self.output_layer.neurons)):\n",
    "            for w_ho in range(len(self.output_layer.neurons[o].weights)):\n",
    "\n",
    "                # ∂Eⱼ/∂wᵢⱼ = ∂E/∂zⱼ * ∂zⱼ/∂wᵢⱼ\n",
    "                pd_error_wrt_weight = pd_errors_wrt_output_neuron_total_net_input[o] * self.output_layer.neurons[o].calculate_pd_total_net_input_wrt_weight(w_ho)\n",
    "\n",
    "                # Δw = α * ∂Eⱼ/∂wᵢ\n",
    "                self.output_layer.neurons[o].weights[w_ho] -= self.LEARNING_RATE * pd_error_wrt_weight\n",
    "\n",
    "        # 4. Update hidden neuron weights\n",
    "        for h in range(len(self.hidden_layer.neurons)):\n",
    "            for w_ih in range(len(self.hidden_layer.neurons[h].weights)):\n",
    "\n",
    "                # ∂Eⱼ/∂wᵢ = ∂E/∂zⱼ * ∂zⱼ/∂wᵢ\n",
    "                pd_error_wrt_weight = pd_errors_wrt_hidden_neuron_total_net_input[h] * self.hidden_layer.neurons[h].calculate_pd_total_net_input_wrt_weight(w_ih)\n",
    "\n",
    "                # Δw = α * ∂Eⱼ/∂wᵢ\n",
    "                self.hidden_layer.neurons[h].weights[w_ih] -= self.LEARNING_RATE * pd_error_wrt_weight\n",
    "\n",
    "    def calculate_total_error(self, training_sets):\n",
    "        total_error = 0\n",
    "        for t in range(len(training_sets)):\n",
    "            training_inputs, training_outputs = training_sets[t]\n",
    "            self.feed_forward(training_inputs)\n",
    "            for o in range(len(training_outputs)):\n",
    "                total_error += self.output_layer.neurons[o].calculate_error(training_outputs[o])\n",
    "        return total_error\n",
    "\n",
    "class NeuronLayer:\n",
    "    def __init__(self, num_neurons, bias):\n",
    "\n",
    "        # Every neuron in a layer shares the same bias\n",
    "        self.bias = bias if bias else random.random()\n",
    "\n",
    "        self.neurons = []\n",
    "        for i in range(num_neurons):\n",
    "            self.neurons.append(Neuron(self.bias))\n",
    "\n",
    "    def inspect(self):\n",
    "        print('Neurons:', len(self.neurons))\n",
    "        for n in range(len(self.neurons)):\n",
    "            print(' Neuron', n)\n",
    "            for w in range(len(self.neurons[n].weights)):\n",
    "                print('  Weight:', self.neurons[n].weights[w])\n",
    "            print('  Bias:', self.bias)\n",
    "\n",
    "    def feed_forward(self, inputs):\n",
    "        outputs = []\n",
    "        for neuron in self.neurons:\n",
    "            outputs.append(neuron.calculate_output(inputs))\n",
    "        return outputs\n",
    "\n",
    "    def get_outputs(self):\n",
    "        outputs = []\n",
    "        for neuron in self.neurons:\n",
    "            outputs.append(neuron.output)\n",
    "        return outputs\n",
    "\n",
    "class Neuron:\n",
    "    def __init__(self, bias):\n",
    "        self.bias = bias\n",
    "        self.weights = []\n",
    "\n",
    "    def calculate_output(self, inputs):\n",
    "        self.inputs = inputs\n",
    "        self.output = self.squash(self.calculate_total_net_input())\n",
    "        return self.output\n",
    "\n",
    "    def calculate_total_net_input(self):\n",
    "        total = 0\n",
    "        for i in range(len(self.inputs)):\n",
    "            total += self.inputs[i] * self.weights[i]\n",
    "        return total + self.bias\n",
    "\n",
    "    # Apply the logistic function to squash the output of the neuron\n",
    "    # The result is sometimes referred to as 'net' [2] or 'net' [1]\n",
    "    def squash(self, total_net_input):\n",
    "        return 1 / (1 + math.exp(-total_net_input))\n",
    "\n",
    "    # Determine how much the neuron's total input has to change to move closer to the expected output\n",
    "    #\n",
    "    # Now that we have the partial derivative of the error with respect to the output (∂E/∂yⱼ) and\n",
    "    # the derivative of the output with respect to the total net input (dyⱼ/dzⱼ) we can calculate\n",
    "    # the partial derivative of the error with respect to the total net input.\n",
    "    # This value is also known as the delta (δ) [1]\n",
    "    # δ = ∂E/∂zⱼ = ∂E/∂yⱼ * dyⱼ/dzⱼ\n",
    "    #\n",
    "    def calculate_pd_error_wrt_total_net_input(self, target_output):\n",
    "        return self.calculate_pd_error_wrt_output(target_output) * self.calculate_pd_total_net_input_wrt_input();\n",
    "\n",
    "    # The error for each neuron is calculated by the Mean Square Error method:\n",
    "    def calculate_error(self, target_output):\n",
    "        return 0.5 * (target_output - self.output) ** 2\n",
    "\n",
    "    # The partial derivate of the error with respect to actual output then is calculated by:\n",
    "    # = 2 * 0.5 * (target output - actual output) ^ (2 - 1) * -1\n",
    "    # = -(target output - actual output)\n",
    "    #\n",
    "    # The Wikipedia article on backpropagation [1] simplifies to the following, but most other learning material does not [2]\n",
    "    # = actual output - target output\n",
    "    #\n",
    "    # Alternative, you can use (target - output), but then need to add it during backpropagation [3]\n",
    "    #\n",
    "    # Note that the actual output of the output neuron is often written as yⱼ and target output as tⱼ so:\n",
    "    # = ∂E/∂yⱼ = -(tⱼ - yⱼ)\n",
    "    def calculate_pd_error_wrt_output(self, target_output):\n",
    "        return -(target_output - self.output)\n",
    "\n",
    "    # The total net input into the neuron is squashed using logistic function to calculate the neuron's output:\n",
    "    # yⱼ = φ = 1 / (1 + e^(-zⱼ))\n",
    "    # Note that where ⱼ represents the output of the neurons in whatever layer we're looking at and ᵢ represents the layer below it\n",
    "    #\n",
    "    # The derivative (not partial derivative since there is only one variable) of the output then is:\n",
    "    # dyⱼ/dzⱼ = yⱼ * (1 - yⱼ)\n",
    "    def calculate_pd_total_net_input_wrt_input(self):\n",
    "        return self.output * (1 - self.output)\n",
    "\n",
    "    # The total net input is the weighted sum of all the inputs to the neuron and their respective weights:\n",
    "    # = zⱼ = netⱼ = x₁w₁ + x₂w₂ ...\n",
    "    #\n",
    "    # The partial derivative of the total net input with respective to a given weight (with everything else held constant) then is:\n",
    "    # = ∂zⱼ/∂wᵢ = some constant + 1 * xᵢw₁^(1-0) + some constant ... = xᵢ\n",
    "    def calculate_pd_total_net_input_wrt_weight(self, index):\n",
    "        return self.inputs[index]\n",
    "\n",
    "###\n",
    "\n",
    "# Blog post example:\n",
    "\n",
    "nn = NeuralNetwork(2, 2, 2, hidden_layer_weights=[0.15, 0.2, 0.25, 0.3], hidden_layer_bias=0.35, output_layer_weights=[0.4, 0.45, 0.5, 0.55], output_layer_bias=0.6)\n",
    "for i in range(1):\n",
    "    nn.train([0.05, 0.1], [0.01, 0.99])\n",
    "    print(i, round(nn.calculate_total_error([[[0.05, 0.1], [0.01, 0.99]]]), 9))\n",
    "\n",
    "# XOR example:\n",
    "\n",
    "# training_sets = [\n",
    "#     [[0, 0], [0]],\n",
    "#     [[0, 1], [1]],\n",
    "#     [[1, 0], [1]],\n",
    "#     [[1, 1], [0]]\n",
    "# ]\n",
    "\n",
    "# nn = NeuralNetwork(len(training_sets[0][0]), 5, len(training_sets[0][1]))\n",
    "# for i in range(1):\n",
    "# #     training_inputs, training_outputs = random.choice(training_sets)\n",
    "#     training_inputs = [0.05,0.1]\n",
    "#     training_outputs = [0.01,0.99]\n",
    "#     nn.train(training_inputs, training_outputs)\n",
    "#     print(i, nn.calculate_total_error(training_sets))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3909695330254254\n",
      "0.5909695330254254\n",
      "0.5020598203245082\n",
      "0.3020598203245082\n",
      "-0.15820738150551855\n",
      "-0.15820738150551855\n",
      "0.014271370040631651\n",
      "0.014271370040631651\n"
     ]
    }
   ],
   "source": [
    "x = [1,1]\n",
    "w_1 = 0.1\n",
    "w_2 = 0.1\n",
    "w_3 = 0.25\n",
    "w_4 = 0.7\n",
    "w_5 = 0.4\n",
    "w_6 = 0.6\n",
    "w_7 = 0.5\n",
    "w_8 = 0.3\n",
    "w = [0.1,0.1,0.2,0.7,0.4,0.6,0.5,0.3]\n",
    "y = [0,1]\n",
    "lr = 0.1\n",
    "def forward(w1,w2,inp1,inp2):\n",
    "    return w1*inp1+w2+inp2\n",
    "def sigmoid(net_sum):\n",
    "    return (1/(1+np.exp(-net_sum)))\n",
    "def sigmoidDerivative(output):\n",
    "    return output*(1.0-output)\n",
    "def backpropgation_out(Error,out,net,weight,target):\n",
    "    error_out = out - target\n",
    "    out_net = sigmoidDerivative(out)\n",
    "    net_weight = out\n",
    "    return weight - lr*error_out*out_net*net_weight\n",
    "def backpropgation_hidden(out_1,out_2,out_h1,weight_1,weight_2,target,x):\n",
    "    error1_out = out_1 - target\n",
    "    error2_out = out_2 - target\n",
    "    out1_net = sigmoidDerivative(out_1)\n",
    "    out2_net = sigmoidDerivative(out_2)\n",
    "    error_hout = (error1_out*out1_net*weight_1+error2_out*out2_net*weight_2)*sigmoidDerivative(out_h1)*x\n",
    "#     error_out = error1_out + error2_out\n",
    "#     out_net = sigmoidDerivative(out)\n",
    "#     net_weight = out\n",
    "    return error_hout\n",
    "net_h1 = forward(w_1,w_2,1,1)\n",
    "out_h1 = sigmoid(net_h1)\n",
    "net_h2 = forward(w_3,w_4,1,1)\n",
    "out_h2 = sigmoid(net_h2)\n",
    "net_o1 = forward(w_5,w_6,out_h1,out_h2)\n",
    "out_o1 = sigmoid(net_o1)\n",
    "net_o2 = forward(w_7,w_8,out_h1,out_h2)\n",
    "out_o2 = sigmoid(net_o2)\n",
    "\n",
    "Error_o1 = 0.5*(0-out_o1)**2\n",
    "Error_o2 = 0.5*(1-out_o2)**2\n",
    "Error_tot = Error_o1 + Error_o2\n",
    "for i in [5,6,7,8]:\n",
    "    if i == 5 or i == 6:\n",
    "        print(backpropgation_out(Error_tot,out_o1,net_o1,w[i-1],0))\n",
    "    else:\n",
    "        print(backpropgation_out(Error_tot,out_o2,net_o2,w[i-1],1))\n",
    "for i in [1,2,3,4]:\n",
    "    if i == 1 or i == 2:\n",
    "        print(backpropgation_hidden(out_o1,out_o2,net_o1,w[4],w[5],0,1))\n",
    "    else:\n",
    "        print(backpropgation_hidden(out_o1,out_o2,net_o2,w[6],w[7],1,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Qestion 3\n",
    "\n",
    "We will need 3 units in the first hidden layer to represent the universal components for both \"F\" and \"L\"; and we will need two units in the second hidden layer to represent the letter \"F\" and \"L\".\n",
    "\n",
    "Please see the following for illustration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "plt.figure(figsize=(5,5))\n",
    "n = 100\n",
    "xy_min = [0, 0]\n",
    "xy_max = [1, 1]\n",
    "train_data = np.random.uniform(low=xy_min, high=xy_max, size=(n,2))\n",
    "plt.scatter(train_data[:,0],train_data[:,1],marker='.')\n",
    "data_square = (train_data[:,0]-0.5)**2 + (train_data[:,1]-0.6)**2\n",
    "right=data_square < 0.4**2\n",
    "wrong=data_square >= 0.4**2\n",
    "True_data=train_data[right]\n",
    "False_data=train_data[wrong]\n",
    "plt.scatter(True_data[:,0],True_data[:,1],marker='.')\n",
    "test_data = np.random.uniform(low=xy_min, high=xy_max, size=(n,2))\n",
    "#plt.scatter(test_data[:,0],test_data[:,1],marker='.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.concatenate((list(True_data),np.ones((len(True_data),1))),axis=1)\n",
    "Train_label = [[list(True_data[i]),[1]] for i in range(len(True_data))]+ \\\n",
    "              [[list(False_data[i]),[0]] for i in range(len(False_data))]\n",
    "len(Train_label) == len(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 19.953239365227414\n",
      "10000 8.860568394214104\n",
      "20000 8.349409881944608\n",
      "30000 7.387550561072061\n",
      "40000 5.230200120233387\n",
      "50000 5.361068885403085\n",
      "60000 4.559150807977866\n",
      "70000 4.608176050713001\n",
      "80000 4.798311109711092\n",
      "90000 5.296893894721095\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import random\n",
    "import math\n",
    "\n",
    "class NeuralNetwork:\n",
    "    LEARNING_RATE = 0.5\n",
    "\n",
    "    def __init__(self, num_inputs, num_hidden, num_outputs, hidden_layer_weights = None, hidden_layer_bias = None, output_layer_weights = None, output_layer_bias = None):\n",
    "        self.num_inputs = num_inputs\n",
    "\n",
    "        self.hidden_layer = NeuronLayer(num_hidden, hidden_layer_bias)\n",
    "        self.output_layer = NeuronLayer(num_outputs, output_layer_bias)\n",
    "\n",
    "        self.init_weights_from_inputs_to_hidden_layer_neurons(hidden_layer_weights)\n",
    "        self.init_weights_from_hidden_layer_neurons_to_output_layer_neurons(output_layer_weights)\n",
    "\n",
    "    def init_weights_from_inputs_to_hidden_layer_neurons(self, hidden_layer_weights):\n",
    "        weight_num = 0\n",
    "        for h in range(len(self.hidden_layer.neurons)):\n",
    "            for i in range(self.num_inputs):\n",
    "                if not hidden_layer_weights:\n",
    "                    self.hidden_layer.neurons[h].weights.append(random.random())\n",
    "                else:\n",
    "                    self.hidden_layer.neurons[h].weights.append(hidden_layer_weights[weight_num])\n",
    "                weight_num += 1\n",
    "\n",
    "    def init_weights_from_hidden_layer_neurons_to_output_layer_neurons(self, output_layer_weights):\n",
    "        weight_num = 0\n",
    "        for o in range(len(self.output_layer.neurons)):\n",
    "            for h in range(len(self.hidden_layer.neurons)):\n",
    "                if not output_layer_weights:\n",
    "                    self.output_layer.neurons[o].weights.append(random.random())\n",
    "                else:\n",
    "                    self.output_layer.neurons[o].weights.append(output_layer_weights[weight_num])\n",
    "                weight_num += 1\n",
    "\n",
    "    def inspect(self):\n",
    "        print('------')\n",
    "        print('* Inputs: {}'.format(self.num_inputs))\n",
    "        print('------')\n",
    "        print('Hidden Layer')\n",
    "        self.hidden_layer.inspect()\n",
    "        print('------')\n",
    "        print('* Output Layer')\n",
    "        self.output_layer.inspect()\n",
    "        print('------')\n",
    "\n",
    "    def feed_forward(self, inputs):\n",
    "        hidden_layer_outputs = self.hidden_layer.feed_forward(inputs)\n",
    "        return self.output_layer.feed_forward(hidden_layer_outputs)\n",
    "\n",
    "    def train(self, training_inputs, training_outputs):\n",
    "        self.feed_forward(training_inputs)\n",
    "\n",
    "        pd_errors_wrt_output_neuron_total_net_input = [0] * len(self.output_layer.neurons)\n",
    "        for o in range(len(self.output_layer.neurons)):\n",
    "\n",
    "            pd_errors_wrt_output_neuron_total_net_input[o] = self.output_layer.neurons[o].calculate_pd_error_wrt_total_net_input(training_outputs[o])\n",
    "\n",
    "        pd_errors_wrt_hidden_neuron_total_net_input = [0] * len(self.hidden_layer.neurons)\n",
    "        for h in range(len(self.hidden_layer.neurons)):\n",
    "\n",
    "            d_error_wrt_hidden_neuron_output = 0\n",
    "            for o in range(len(self.output_layer.neurons)):\n",
    "                d_error_wrt_hidden_neuron_output += pd_errors_wrt_output_neuron_total_net_input[o] * self.output_layer.neurons[o].weights[h]\n",
    "\n",
    "            pd_errors_wrt_hidden_neuron_total_net_input[h] = d_error_wrt_hidden_neuron_output * self.hidden_layer.neurons[h].calculate_pd_total_net_input_wrt_input()\n",
    "\n",
    "        for o in range(len(self.output_layer.neurons)):\n",
    "            for w_ho in range(len(self.output_layer.neurons[o].weights)):\n",
    "\n",
    "                pd_error_wrt_weight = pd_errors_wrt_output_neuron_total_net_input[o] * self.output_layer.neurons[o].calculate_pd_total_net_input_wrt_weight(w_ho)\n",
    "\n",
    "                self.output_layer.neurons[o].weights[w_ho] -= self.LEARNING_RATE * pd_error_wrt_weight\n",
    "\n",
    "        for h in range(len(self.hidden_layer.neurons)):\n",
    "            for w_ih in range(len(self.hidden_layer.neurons[h].weights)):\n",
    "\n",
    "                pd_error_wrt_weight = pd_errors_wrt_hidden_neuron_total_net_input[h] * self.hidden_layer.neurons[h].calculate_pd_total_net_input_wrt_weight(w_ih)\n",
    "\n",
    "                self.hidden_layer.neurons[h].weights[w_ih] -= self.LEARNING_RATE * pd_error_wrt_weight\n",
    "    \n",
    "    def test(self, test_inputs):\n",
    "        return self.feed_forward(test_inputs)\n",
    "    \n",
    "    def calculate_total_error(self, training_sets):\n",
    "        total_error = 0\n",
    "        for t in range(len(training_sets)):\n",
    "            training_inputs, training_outputs = training_sets[t]\n",
    "            self.feed_forward(training_inputs)\n",
    "            for o in range(len(training_outputs)):\n",
    "                total_error += self.output_layer.neurons[o].calculate_error(training_outputs[o])\n",
    "        return total_error\n",
    "\n",
    "class NeuronLayer:\n",
    "    def __init__(self, num_neurons, bias):\n",
    "\n",
    "        # Every neuron in a layer shares the same bias\n",
    "        self.bias = bias if bias else random.random()\n",
    "\n",
    "        self.neurons = []\n",
    "        for i in range(num_neurons):\n",
    "            self.neurons.append(Neuron(self.bias))\n",
    "\n",
    "    def inspect(self):\n",
    "        print('Neurons:', len(self.neurons))\n",
    "        for n in range(len(self.neurons)):\n",
    "            print(' Neuron', n)\n",
    "            for w in range(len(self.neurons[n].weights)):\n",
    "                print('  Weight:', self.neurons[n].weights[w])\n",
    "            print('  Bias:', self.bias)\n",
    "\n",
    "    def feed_forward(self, inputs):\n",
    "        outputs = []\n",
    "        for neuron in self.neurons:\n",
    "            outputs.append(neuron.calculate_output(inputs))\n",
    "        return outputs\n",
    "\n",
    "    def get_outputs(self):\n",
    "        outputs = []\n",
    "        for neuron in self.neurons:\n",
    "            outputs.append(neuron.output)\n",
    "        return outputs\n",
    "\n",
    "class Neuron:\n",
    "    def __init__(self, bias):\n",
    "        self.bias = bias\n",
    "        self.weights = []\n",
    "\n",
    "    def calculate_output(self, inputs):\n",
    "        self.inputs = inputs\n",
    "        self.output = self.squash(self.calculate_total_net_input())\n",
    "        return self.output\n",
    "\n",
    "    def calculate_total_net_input(self):\n",
    "        total = 0\n",
    "        for i in range(len(self.inputs)):\n",
    "            total += self.inputs[i] * self.weights[i]\n",
    "        return total + self.bias\n",
    "\n",
    "    def squash(self, total_net_input):\n",
    "        return 1 / (1 + math.exp(-total_net_input))\n",
    "\n",
    "    def calculate_pd_error_wrt_total_net_input(self, target_output):\n",
    "        return self.calculate_pd_error_wrt_output(target_output) * self.calculate_pd_total_net_input_wrt_input();\n",
    "\n",
    "    # The error for each neuron is calculated by the Mean Square Error method:\n",
    "    def calculate_error(self, target_output):\n",
    "        return 0.5 * (target_output - self.output) ** 2\n",
    "\n",
    "    def calculate_pd_error_wrt_output(self, target_output):\n",
    "        return -(target_output - self.output)\n",
    "\n",
    "    def calculate_pd_total_net_input_wrt_input(self):\n",
    "        return self.output * (1 - self.output)\n",
    "\n",
    "    def calculate_pd_total_net_input_wrt_weight(self, index):\n",
    "        return self.inputs[index]\n",
    "\n",
    "###\n",
    "\n",
    "# mini example:\n",
    "\n",
    "# nn = NeuralNetwork(2, 2, 2, hidden_layer_weights=[0.15, 0.2, 0.25, 0.3], hidden_layer_bias=0.35, output_layer_weights=[0.4, 0.45, 0.5, 0.55], output_layer_bias=0.6)\n",
    "# for i in range(1):\n",
    "#     nn.train([0.05, 0.1], [0.01, 0.99])\n",
    "#     print(i, round(nn.calculate_total_error([[[0.05, 0.1], [0.01, 0.99]]]), 9))\n",
    "\n",
    "# circle recognition example:\n",
    "\n",
    "training_sets = Train_label\n",
    "\n",
    "nn = NeuralNetwork(len(training_sets[0][0]), 10, len(training_sets[0][1]))\n",
    "for i in range(100000):\n",
    "    training_inputs, training_outputs = random.choice(training_sets)\n",
    "#     training_inputs = [0.05,0.1]\n",
    "#     training_outputs = [0.01,0.99]\n",
    "    nn.train(training_inputs, training_outputs)\n",
    "    if i % 10000==0:\n",
    "        print(i, nn.calculate_total_error(training_sets))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False True\n",
      "False False\n",
      "False False\n",
      "False False\n",
      "False False\n",
      "False False\n",
      "True True\n",
      "True True\n",
      "True True\n",
      "True True\n",
      "False False\n",
      "True True\n",
      "True True\n",
      "True True\n",
      "False False\n",
      "False False\n",
      "True True\n",
      "False False\n",
      "False False\n",
      "False False\n",
      "True True\n",
      "True True\n",
      "True True\n",
      "True False\n",
      "True True\n",
      "False False\n",
      "False False\n",
      "True False\n",
      "True False\n",
      "True True\n",
      "False False\n",
      "True False\n",
      "True True\n",
      "False False\n",
      "True True\n",
      "True True\n",
      "True False\n",
      "True True\n",
      "True True\n",
      "True True\n",
      "False False\n",
      "True True\n",
      "True True\n",
      "True True\n",
      "True True\n",
      "False False\n",
      "True True\n",
      "True True\n",
      "True True\n",
      "True True\n",
      "False False\n",
      "True True\n",
      "False False\n",
      "False False\n",
      "True True\n",
      "True False\n",
      "False False\n",
      "False False\n",
      "False False\n",
      "True False\n",
      "False False\n",
      "True True\n",
      "False False\n",
      "False False\n",
      "False False\n",
      "False False\n",
      "False False\n",
      "True True\n",
      "False False\n",
      "False False\n",
      "True True\n",
      "True True\n",
      "False False\n",
      "False False\n",
      "False False\n",
      "False False\n",
      "False False\n",
      "True True\n",
      "True True\n",
      "True True\n",
      "True True\n",
      "True False\n",
      "True False\n",
      "True True\n",
      "False False\n",
      "False False\n",
      "False False\n",
      "True True\n",
      "True True\n",
      "False False\n",
      "True False\n",
      "True True\n",
      "False False\n",
      "False False\n",
      "True True\n",
      "True False\n",
      "False False\n",
      "True True\n",
      "True True\n",
      "False False\n"
     ]
    }
   ],
   "source": [
    "for i,v in enumerate(test_data):\n",
    "    a = nn.test(v)\n",
    "    if abs(a[0]-1) < abs(a[0]):\n",
    "        a = True\n",
    "    else:\n",
    "        a = False\n",
    "    print(a,right[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ True, False, False, False, False, False,  True,  True,  True,\n",
       "        True, False,  True,  True,  True, False, False,  True, False,\n",
       "       False, False,  True,  True,  True, False,  True, False, False,\n",
       "       False, False,  True, False, False,  True, False,  True,  True,\n",
       "       False,  True,  True,  True, False,  True,  True,  True,  True,\n",
       "       False,  True,  True,  True,  True, False,  True, False, False,\n",
       "        True, False, False, False, False, False, False,  True, False,\n",
       "       False, False, False, False,  True, False, False,  True,  True,\n",
       "       False, False, False, False, False,  True,  True,  True,  True,\n",
       "       False, False,  True, False, False, False,  True,  True, False,\n",
       "       False,  True, False, False,  True, False, False,  True,  True,\n",
       "       False])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_square = (test_data[:,0]-0.5)**2 + (test_data[:,1]-0.6)**2\n",
    "right=data_square < 0.4**2\n",
    "wrong=data_square >= 0.4**2\n",
    "True_data=test_data[right]\n",
    "False_data=test_data[wrong]\n",
    "right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
